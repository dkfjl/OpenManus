"""
æç¤ºè¯åº“æ€§èƒ½æµ‹è¯•è„šæœ¬
ä½¿ç”¨ pytest-benchmark è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import time
import statistics
from typing import List, Dict
from fastapi.testclient import TestClient

from app.app import app
from app.services.prompt_service import PromptService
from app.services.prompt_storage import PromptStorage

# åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
client = TestClient(app)

# æµ‹è¯•ç”¨æˆ·ID
TEST_USER = "perf_test_user"


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.latencies: List[float] = []

    def record(self, latency_ms: float):
        """è®°å½•ä¸€æ¬¡è¯·æ±‚çš„å»¶è¿Ÿ"""
        self.latencies.append(latency_ms)

    def get_percentile(self, p: int) -> float:
        """è·å–ç™¾åˆ†ä½æ•°"""
        if not self.latencies:
            return 0.0
        sorted_latencies = sorted(self.latencies)
        index = int(len(sorted_latencies) * p / 100)
        return sorted_latencies[min(index, len(sorted_latencies) - 1)]

    def get_stats(self) -> Dict[str, float]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.latencies:
            return {}

        return {
            "count": len(self.latencies),
            "min": min(self.latencies),
            "max": max(self.latencies),
            "mean": statistics.mean(self.latencies),
            "median": statistics.median(self.latencies),
            "p50": self.get_percentile(50),
            "p90": self.get_percentile(90),
            "p95": self.get_percentile(95),
            "p99": self.get_percentile(99),
        }


def setup_test_data(count: int = 100) -> List[str]:
    """å‡†å¤‡æµ‹è¯•æ•°æ®"""
    print(f"\nğŸ“¦ å‡†å¤‡ {count} æ¡æµ‹è¯•æ•°æ®...")
    prompt_ids = []

    for i in range(count):
        response = client.post(
            "/console/api/prompts",
            json={
                "name": f"æ€§èƒ½æµ‹è¯•æç¤ºè¯_{i}",
                "description": f"ç”¨äºæ€§èƒ½æµ‹è¯•çš„æç¤ºè¯ #{i}",
                "prompt": f"ä½ æ˜¯{{role}}ï¼Œä½ çš„ä»»åŠ¡æ˜¯{{task}}ã€‚è¿™æ˜¯ç¬¬ {i} æ¡æµ‹è¯•æ•°æ®ã€‚",
                "ownerId": TEST_USER
            },
            headers={"X-User-Id": TEST_USER}
        )

        if response.status_code == 201:
            data = response.json()
            prompt_ids.append(data["data"]["id"])

    print(f"   âœ… æˆåŠŸåˆ›å»º {len(prompt_ids)} æ¡æµ‹è¯•æ•°æ®")
    return prompt_ids


def cleanup_test_data(prompt_ids: List[str]):
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    print(f"\nğŸ§¹ æ¸…ç† {len(prompt_ids)} æ¡æµ‹è¯•æ•°æ®...")
    for prompt_id in prompt_ids:
        client.delete(
            f"/console/api/prompts/{prompt_id}",
            headers={"X-User-Id": TEST_USER}
        )
    print(f"   âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†")


def test_list_recommended_prompts(iterations: int = 100):
    """æµ‹è¯•ï¼šåˆ—å‡ºæ¨èæ¨¡æ¿æ€§èƒ½"""
    print(f"\nğŸ§ª æµ‹è¯• 1: åˆ—å‡ºæ¨èæ¨¡æ¿ ({iterations} æ¬¡)")

    metrics = PerformanceMetrics()

    for _ in range(iterations):
        start = time.time()
        response = client.get("/console/api/prompt/overview", params={
            "type": "recommended",
            "page": 1,
            "pageSize": 20
        })
        latency_ms = (time.time() - start) * 1000
        metrics.record(latency_ms)

        assert response.status_code == 200, f"è¯·æ±‚å¤±è´¥: {response.status_code}"

    stats = metrics.get_stats()
    print(f"   ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"      - è¯·æ±‚æ¬¡æ•°: {stats['count']}")
    print(f"      - å¹³å‡å»¶è¿Ÿ: {stats['mean']:.2f} ms")
    print(f"      - P50: {stats['p50']:.2f} ms")
    print(f"      - P95: {stats['p95']:.2f} ms")
    print(f"      - P99: {stats['p99']:.2f} ms")

    # éªŒè¯æ€§èƒ½ç›®æ ‡ï¼šP50 < 150ms
    if stats['p50'] < 150:
        print(f"   âœ… æ€§èƒ½è¾¾æ ‡ï¼P50 ({stats['p50']:.2f} ms) < 150 ms")
    else:
        print(f"   âš ï¸ æ€§èƒ½æœªè¾¾æ ‡ï¼P50 ({stats['p50']:.2f} ms) >= 150 ms")

    return stats


def test_get_prompt_detail(prompt_ids: List[str], iterations: int = 100):
    """æµ‹è¯•ï¼šè·å–æç¤ºè¯è¯¦æƒ…æ€§èƒ½"""
    print(f"\nğŸ§ª æµ‹è¯• 2: è·å–æç¤ºè¯è¯¦æƒ… ({iterations} æ¬¡)")

    if not prompt_ids:
        print("   âš ï¸ è·³è¿‡ï¼šæ²¡æœ‰æµ‹è¯•æ•°æ®")
        return

    metrics = PerformanceMetrics()

    for i in range(iterations):
        # å¾ªç¯ä½¿ç”¨æµ‹è¯•æ•°æ®
        prompt_id = prompt_ids[i % len(prompt_ids)]

        start = time.time()
        response = client.get("/console/api/prompt/detail", params={
            "type": "personal",
            "id": prompt_id
        }, headers={"X-User-Id": TEST_USER})
        latency_ms = (time.time() - start) * 1000
        metrics.record(latency_ms)

        assert response.status_code == 200, f"è¯·æ±‚å¤±è´¥: {response.status_code}"

    stats = metrics.get_stats()
    print(f"   ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"      - è¯·æ±‚æ¬¡æ•°: {stats['count']}")
    print(f"      - å¹³å‡å»¶è¿Ÿ: {stats['mean']:.2f} ms")
    print(f"      - P50: {stats['p50']:.2f} ms")
    print(f"      - P95: {stats['p95']:.2f} ms")
    print(f"      - P99: {stats['p99']:.2f} ms")

    # éªŒè¯æ€§èƒ½ç›®æ ‡ï¼šP50 < 150ms
    if stats['p50'] < 150:
        print(f"   âœ… æ€§èƒ½è¾¾æ ‡ï¼P50 ({stats['p50']:.2f} ms) < 150 ms")
    else:
        print(f"   âš ï¸ æ€§èƒ½æœªè¾¾æ ‡ï¼P50 ({stats['p50']:.2f} ms) >= 150 ms")

    return stats


def test_list_personal_prompts(iterations: int = 100):
    """æµ‹è¯•ï¼šåˆ—å‡ºä¸ªäººæç¤ºè¯æ€§èƒ½"""
    print(f"\nğŸ§ª æµ‹è¯• 3: åˆ—å‡ºä¸ªäººæç¤ºè¯ ({iterations} æ¬¡)")

    metrics = PerformanceMetrics()

    for _ in range(iterations):
        start = time.time()
        response = client.get("/console/api/prompt/overview", params={
            "type": "personal",
            "page": 1,
            "pageSize": 20
        }, headers={"X-User-Id": TEST_USER})
        latency_ms = (time.time() - start) * 1000
        metrics.record(latency_ms)

        assert response.status_code == 200, f"è¯·æ±‚å¤±è´¥: {response.status_code}"

    stats = metrics.get_stats()
    print(f"   ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"      - è¯·æ±‚æ¬¡æ•°: {stats['count']}")
    print(f"      - å¹³å‡å»¶è¿Ÿ: {stats['mean']:.2f} ms")
    print(f"      - P50: {stats['p50']:.2f} ms")
    print(f"      - P95: {stats['p95']:.2f} ms")
    print(f"      - P99: {stats['p99']:.2f} ms")

    # éªŒè¯æ€§èƒ½ç›®æ ‡ï¼šP50 < 150ms
    if stats['p50'] < 150:
        print(f"   âœ… æ€§èƒ½è¾¾æ ‡ï¼P50 ({stats['p50']:.2f} ms) < 150 ms")
    else:
        print(f"   âš ï¸ æ€§èƒ½æœªè¾¾æ ‡ï¼P50 ({stats['p50']:.2f} ms) >= 150 ms")

    return stats


def test_create_prompt_performance(iterations: int = 50):
    """æµ‹è¯•ï¼šåˆ›å»ºæç¤ºè¯æ€§èƒ½"""
    print(f"\nğŸ§ª æµ‹è¯• 4: åˆ›å»ºæç¤ºè¯ ({iterations} æ¬¡)")

    metrics = PerformanceMetrics()
    created_ids = []

    for i in range(iterations):
        start = time.time()
        response = client.post(
            "/console/api/prompts",
            json={
                "name": f"æ€§èƒ½æµ‹è¯•åˆ›å»º_{i}",
                "description": "æ€§èƒ½æµ‹è¯•",
                "prompt": "æµ‹è¯•å†…å®¹",
                "ownerId": TEST_USER
            },
            headers={"X-User-Id": TEST_USER}
        )
        latency_ms = (time.time() - start) * 1000
        metrics.record(latency_ms)

        if response.status_code == 201:
            created_ids.append(response.json()["data"]["id"])

    stats = metrics.get_stats()
    print(f"   ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"      - è¯·æ±‚æ¬¡æ•°: {stats['count']}")
    print(f"      - å¹³å‡å»¶è¿Ÿ: {stats['mean']:.2f} ms")
    print(f"      - P50: {stats['p50']:.2f} ms")
    print(f"      - P95: {stats['p95']:.2f} ms")
    print(f"      - P99: {stats['p99']:.2f} ms")

    # æ¸…ç†åˆ›å»ºçš„æ•°æ®
    for prompt_id in created_ids:
        client.delete(
            f"/console/api/prompts/{prompt_id}",
            headers={"X-User-Id": TEST_USER}
        )

    return stats


def test_service_layer_performance():
    """æµ‹è¯•ï¼šService å±‚ç›´æ¥è°ƒç”¨æ€§èƒ½"""
    print(f"\nğŸ§ª æµ‹è¯• 5: Service å±‚æ€§èƒ½ï¼ˆ100 æ¬¡ï¼‰")

    service = PromptService()
    metrics = PerformanceMetrics()

    # æµ‹è¯•åˆ—è¡¨æ¨èæ¨¡æ¿
    for _ in range(100):
        start = time.time()
        service.list_prompts(prompt_type="recommended", page=1, page_size=20)
        latency_ms = (time.time() - start) * 1000
        metrics.record(latency_ms)

    stats = metrics.get_stats()
    print(f"   ğŸ“Š Service å±‚ç»Ÿè®¡:")
    print(f"      - å¹³å‡å»¶è¿Ÿ: {stats['mean']:.2f} ms")
    print(f"      - P50: {stats['p50']:.2f} ms")
    print(f"      - P95: {stats['p95']:.2f} ms")

    if stats['p50'] < 50:
        print(f"   âœ… Service å±‚æ€§èƒ½ä¼˜ç§€ï¼P50 ({stats['p50']:.2f} ms) < 50 ms")
    else:
        print(f"   âš ï¸ Service å±‚æ€§èƒ½éœ€è¦ä¼˜åŒ–")

    return stats


def run_all_performance_tests():
    """æ‰§è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•")
    print("=" * 60)

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data_ids = setup_test_data(100)

    try:
        # è¿è¡Œæµ‹è¯•
        test_list_recommended_prompts(100)
        test_get_prompt_detail(test_data_ids, 100)
        test_list_personal_prompts(100)
        test_create_prompt_performance(50)
        test_service_layer_performance()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)

    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        cleanup_test_data(test_data_ids)


if __name__ == "__main__":
    run_all_performance_tests()
