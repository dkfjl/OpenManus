# Global LLM configuration
[llm]
model = "claude-3-7-sonnet-20250219"       # The LLM model to use
base_url = "https://api.anthropic.com/v1/" # API endpoint URL
api_key = "YOUR_API_KEY"                   # Your API key
max_tokens = 8192                          # Maximum number of tokens in the response
temperature = 0.0                          # Controls randomness

# [llm] # Amazon Bedrock
# api_type = "aws"                                       # Required
# model = "us.anthropic.claude-3-7-sonnet-20250219-v1:0" # Bedrock supported modelID
# base_url = "bedrock-runtime.us-west-2.amazonaws.com"   # Not used now
# max_tokens = 8192
# temperature = 1.0
# api_key = "bear"                                       # Required but not used for Bedrock

# [llm] #AZURE OPENAI:
# api_type= 'azure'
# model = "YOUR_MODEL_NAME" #"gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPLOYMENT_ID}"
# api_key = "AZURE API KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version="AZURE API VERSION" #"2024-08-01-preview"

# [llm] #OLLAMA:
# api_type = 'ollama'
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# [llm] #Jiekou.AI:
# api_type = 'jiekou'
# model = "claude-sonnet-4-5-20250929"                               # The LLM model to use
# base_url = "https://api.jiekou.ai/openai"                          # API endpoint URL
# api_key = "your Jiekou.AI api key"                                 # Your API key
# max_tokens = 64000                                                 # Maximum number of tokens in the response
# temperature = 0.0                                                  # Controls randomness

# Optional configuration for specific LLM models
[llm.vision]
model = "claude-3-7-sonnet-20250219"       # The vision model to use
base_url = "https://api.anthropic.com/v1/" # API endpoint URL for vision model
api_key = "YOUR_API_KEY"                   # Your API key for vision model
max_tokens = 8192                          # Maximum number of tokens in the response
temperature = 0.0                          # Controls randomness for vision model

# [llm.vision] #OLLAMA VISION:
# api_type = 'ollama'
# model = "llama3.2-vision"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# Optional configuration for specific browser configuration
# [browser]
# Whether to run browser in headless mode (default: false)
#headless = false
# Disable browser security features (default: true)
#disable_security = true
# Extra arguments to pass to the browser
#extra_chromium_args = []
# Path to a Chrome instance to use to connect to your normal browser
# e.g. '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
#chrome_instance_path = ""
# Connect to a browser instance via WebSocket
#wss_url = ""
# Connect to a browser instance via CDP
#cdp_url = ""

# Optional configuration, Proxy settings for the browser
# [browser.proxy]
# server = "http://proxy-server:port"
# username = "proxy-username"
# password = "proxy-password"

# Optional configuration, Search settings.
# [search]
# Search engine for agent to use. Options: "Google" | "DuckDuckGo" | "Bing" | "Baidu".
# engine = "Google"
# Explicit fallback order. When set, ONLY these engines are tried after the primary.
# fallback_engines = ["DuckDuckGo", "Bing"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
#retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
#max_retries = 3
# Language code for search results. Options: "en" (English), "zh" (Chinese), etc.
#lang = "en"
# Country code for search results. Options: "us" (United States), "cn" (China), etc.
#country = "us"

# DuckDuckGo parameters (optional)
# Examples: region: wt-wt (worldwide), us-en, cn-zh; safesearch: off|moderate|strict; timelimit: d|w|m|y
# ddg_region = "wt-wt"
# ddg_safesearch = "moderate"
# ddg_timelimit = ""
# ddg_backend = "api"  # api|html|lite
# Optional: Use external DuckDuckGo API (e.g., RapidAPI)
# ddg_use_api = false
# ddg_api_key = "YOUR_RAPIDAPI_KEY"
# ddg_api_endpoint = "https://duckduckgo10.p.rapidapi.com/search"
# ddg_api_host = "duckduckgo10.p.rapidapi.com"

# Bing parameters (optional)
# Market and localization: mkt (en-US, zh-CN), cc (US, CN), lang (en, zh-CN)
# SafeSearch: off|moderate|strict
# bing_mkt = "en-US"
# bing_cc = "US"
# bing_lang = "en"
# bing_safesearch = "moderate"

# Use official Bing Web Search API (optional)
# bing_use_api = false
# bing_api_key = "YOUR_BING_WEB_SEARCH_KEY"
# bing_api_endpoint = "https://api.bing.microsoft.com/v7.0/search"

# Bocha AI Search configuration (optional)
# Get your API key from https://open.bocha.cn
# bocha_api_key = "your_bocha_api_key_here"
# bocha_freshness = "noLimit"  # Options: noLimit, oneDay, oneWeek, oneMonth, oneYear
# bocha_summary = true  # Include summary in search results
# bocha_include = "qq.com|m.163.com"  # Specify websites to include (domains separated by |)
# bocha_exclude = "qq.com|m.163.com"  # Specify websites to exclude (domains separated by |)

# Optional configuration, Image search settings.
#[image_search]
# provider_priority = ["google_cse", "playwright"]
# google_api_key = "YOUR_GOOGLE_API_KEY"   # Required for Google CSE
# google_cx = "YOUR_GOOGLE_CSE_CX"         # Custom Search Engine ID
# google_endpoint = "https://www.googleapis.com/customsearch/v1"
# google_safe = "medium"                    # off | medium | high
#
# # (Optional legacy) If you want to use Bing instead:
# # provider_priority = ["bing_api", "playwright"]
# # bing_api_key = "YOUR_BING_IMAGE_SEARCH_KEY"
# # bing_endpoint = "https://api.bing.microsoft.com/v7.0/images/search"
# # market = "en-US"
# # safe_search = "Moderate"                # Off | Moderate | Strict


## Sandbox configuration
#[sandbox]
#use_sandbox = false
#image = "python:3.12-slim"
#work_dir = "/workspace"
#memory_limit = "1g"  # 512m
#cpu_limit = 2.0
#timeout = 300
#network_enabled = true

# MCP (Model Context Protocol) configuration
[mcp]
server_reference = "app.mcp.server" # default server module reference

# Optional Runflow configuration
# Your can add additional agents into run-flow workflow to solve different-type tasks.
[runflow]
use_data_analysis_agent = false     # The Data Analysi Agent to solve various data analysis tasks

# AIPPT configuration for PPT generation
[aippt]
base_url = "http://192.168.1.119:3001"  # AIPPT API base URL
request_timeout = 300                      # Request timeout in seconds
default_style = "通用"                      # Default PPT style (通用/学术风/职场风/教育风/营销风)
default_model = "gemini-3-pro-preview"     # Default AI model
